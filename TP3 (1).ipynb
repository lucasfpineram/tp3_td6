{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1LYdorEvtbS"
      },
      "source": [
        "\n",
        "# TP3: Clasificador de imagenes\n",
        "\n",
        "\n",
        "``torchvision`` contiene  data loaders para datasets conocidos como ImageNet, CIFAR10, MNIST, etc.\n",
        "``torchvision.datasets`` and ``torch.utils.data.DataLoader``.\n",
        "\n",
        "El dataset CIFAR10 contiene las siguientes clases: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’,\n",
        "‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’.\n",
        "Las imagenes son de tamaño 3x32x32, i.e. imagenes de color de 3 canales (RGB) de 32x32 pixels en tamaño.\n",
        "\n",
        "\n",
        "## Orden de pasos\n",
        "\n",
        "0. Elijan GPU para que corra mas rapido (RAM --> change runtime type --> T4 GPU)\n",
        "1. Cargamos los datos CIFAR10 y preparamos los sets de training, validation y test.\n",
        "2. Definimos las redes. Una MLP, un una CNN\n",
        "3. Definimos la loss\n",
        "4. Entrenamos\n",
        "5. Evaluamos en Test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7GXDX7uuwll"
      },
      "source": [
        "## FLAGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wQFexzFIuwFj"
      },
      "outputs": [],
      "source": [
        "# EJERCICIOS\n",
        "arquitectura = False #2\n",
        "arquitectura_cnn = False #3\n",
        "activacion_experiment = False #4\n",
        "optimizar =  activacion_experiment #5\n",
        "mejoras_lr = True #6\n",
        "regularizadores = False #7\n",
        "\n",
        "\n",
        "# AUXILIARES\n",
        "data_augmentation = True\n",
        "el_mejor = False #8\n",
        "el_peor = False #8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wnBoHA7ii9EA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def generar_metadata(net,experiment_name,epochs,learning_rate,batch_size,momentum,optimizer, scheduler1,loss,train_acc,train_loss,val_acc,val_loss,data_augmentation,dropout,weight_decay,regularizadores):\n",
        "    metadata = {\n",
        "        'experiment_name': experiment_name,\n",
        "        'epochs': epochs,\n",
        "        'learning_rate': learning_rate,\n",
        "        'batch_size': batch_size,\n",
        "        'momentum': momentum,\n",
        "        'optimizer': optimizer.__class__.__name__,\n",
        "        'scheduler1': scheduler1,\n",
        "        'loss': loss,\n",
        "        'train_acc': train_acc,\n",
        "        'train_loss': train_loss,\n",
        "        'val_acc': val_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'regularizadores':regularizadores\n",
        "    }\n",
        "    return metadata\n",
        "\n",
        "def save_data_to_json(metadata,path):\n",
        "    with open(path,'w') as f:\n",
        "        json.dump(metadata,f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3MW_SgnvtbY"
      },
      "source": [
        "## 1. Configuración\n",
        "Crearse una cuenta en Weights and Biases (o en su board favorito) y linkear la notebook a este. Cada experimento deberá contener nombres dicientes y almacenar los (hiper)parámetros de configuración del mismo. Deberá separar los sets de datos en entrenamiento, validación y test. Utilizando solamente los sets de entrenamiento y validación, registrar la loss en train y validación en cada iteración que considere conveniente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09qrFEI4vtba",
        "outputId": "01906c30-ccf0-4f50-a6b8-c4ad6a8b1512"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.12)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.34.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg5t6WnXoi1Z",
        "outputId": "917b17e1-6667-4b93-92c4-9a3c92be4c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z06nOSDXvtbc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import v2\n",
        "from torchsummary import summary\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, ExponentialLR,MultiStepLR\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4O_5JXHvtbe",
        "outputId": "f94cb9f2-7db0-46a5-e598-7f59d99577a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-Vwwcxnvtbe",
        "outputId": "c6c85db9-d304-419a-c2fa-ebba53c74887"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mosaintnom\u001b[0m (\u001b[33mchicas_superpoderosas\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !wandb login --relogin\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-yuBpK8vtbf"
      },
      "source": [
        "bfbad9b2649155692b5f97a49a43c0eeb66dff4a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qLjhp9tevtbf"
      },
      "outputs": [],
      "source": [
        "# Chequeamos si la carpeta data existe, si no existe la creamos\n",
        "download = not os.path.isdir('./data')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Pl99kfRQvtbg"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(181988)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(181988)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Vhe4FrtLvtbg"
      },
      "outputs": [],
      "source": [
        "#Parametros\n",
        "# batch_size = 32\n",
        "# learning_rate = 0.001\n",
        "# momentum =0.8\n",
        "# epochs = 10\n",
        "# weight_decay = 1e-5\n",
        "\n",
        "# num_classes = 10\n",
        "\n",
        "\n",
        "# experiment_name = \"MLP\"\n",
        "# experiment_name = \"fc_hl_bigvalues\"\n",
        "# experiment_name = \"fc_hl_1layers\"\n",
        "# experiment_name = \"fc_hl_values_and_2layers\"\n",
        "# experiment_name = \"fc_hl_bigvalues_and_2layers\"\n",
        "# experiment_name = \"fc_hl_less_values\"\n",
        "# experiment_name = \"fc_hl_1layer_less\"\n",
        "# experiment_name = \"fc_hl_1layerless_smallvalues\"\n",
        "# experiment_name = \"cnn_lessvalues\"\n",
        "# experiment_name = \"cnn_lessvalues_LR_MP\"\n",
        "# experiment_name = 'CNN_LeNet-5'\n",
        "# experiment_name = 'CNN_ALexNet'\n",
        "# experiment_name = 'CNN_VGG16'\n",
        "experiment_name = 'CNN_ResNet-18'\n",
        "\n",
        "\n",
        "project_name = \"TP3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uq31NdHWvtbg"
      },
      "outputs": [],
      "source": [
        "name_deep_net_exp = ['MLP','fc_hl_bigvalues','fc_hl_1layers','fc_hl_values_and_2layers','fc_hl_bigvalues_and_2layers','fc_hl_less_values','fc_hl_1layer_less','fc_hl_1layerless_smallvalues','cnn_lessvalues','cnn_lessvalues_LR_MP']\n",
        "\n",
        "name_conv_net_exp = ['CNN_LeNet-5','CNN_ALexNet','CNN_ResNet-18','CNN_VGG16']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AfU0X5SYvtbh"
      },
      "outputs": [],
      "source": [
        "# Guardamos en la variable transform un tensor de 3 dimensiones (RGB) normalizado\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDRB5woUshmo"
      },
      "source": [
        "###7 data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBU3BFMy8JjZ"
      },
      "source": [
        "Los cambios que le realizamos a las imagenes son las siguientes:\n",
        "\n",
        "* Random Crop\n",
        "* Flip left-right\n",
        "* Cutout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uWPXafFe8InB"
      },
      "outputs": [],
      "source": [
        "mean, std = [0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]\n",
        "# mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
        "\n",
        "if data_augmentation:\n",
        "    composed_train = transforms.Compose([\n",
        "        transforms.Pad(4),\n",
        "        transforms.RandomCrop(32),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "        transforms.RandomErasing(scale=(0.0625, 0.0625), ratio=(1.0, 1.0))\n",
        "    ])\n",
        "\n",
        "    composed_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H8TvZomvtbh"
      },
      "source": [
        "### Train Set y Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dUzdqitrw6mc"
      },
      "outputs": [],
      "source": [
        "    def create_trainset(root='./data', transform=None):\n",
        "        trainset = torchvision.datasets.CIFAR10(root=root, train=True, download=True, transform=transform)\n",
        "        return trainset\n",
        "\n",
        "    def create_loaders(trainset, batch_size, num_workers=2):\n",
        "        targets_ = trainset.targets\n",
        "        train_idx, val_idx = train_test_split(np.arange(len(targets_)), test_size=0.2, stratify=targets_)\n",
        "        train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "        val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(trainset, sampler=train_sampler, batch_size=batch_size, num_workers=num_workers)\n",
        "        valloader = torch.utils.data.DataLoader(trainset, sampler=val_sampler, batch_size=batch_size, num_workers=num_workers)\n",
        "\n",
        "        return trainloader, valloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "t63CYkKTvtbh"
      },
      "outputs": [],
      "source": [
        "if not data_augmentation:\n",
        "    trainset = create_trainset(root='./data', transform=transform)\n",
        "    trainloader, valloader = create_loaders(trainset, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "tZ8nclD2tUub",
        "outputId": "dac0f4b2-56c3-450a-8864-bd35bc325b17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e60d3897867d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_trainset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomposed_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
          ]
        }
      ],
      "source": [
        "if data_augmentation:\n",
        "    trainset = create_trainset(root='./data', transform=composed_train)\n",
        "    trainloader, valloader = create_loaders(trainset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBMXuN4Gvtbi"
      },
      "source": [
        "### Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm3Qkwc_vtbi"
      },
      "outputs": [],
      "source": [
        "if not data_augmentation:\n",
        "  testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "  classes = ('plane', 'car', 'bird', 'cat',\n",
        "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGe-p9rttcYO"
      },
      "outputs": [],
      "source": [
        "if data_augmentation:\n",
        "  testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=composed_test)\n",
        "  testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "  classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dflI3lY5vtbi"
      },
      "source": [
        "### Visualizacion de Imagenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47WIfOcxsVLe"
      },
      "outputs": [],
      "source": [
        "def show_data(img):\n",
        "    try:\n",
        "        plt.imshow(img[0])\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "    print(img[0].shape, img[0].permute(1,2,0).shape)\n",
        "    plt.imshow(img[0].permute(1,2,0))\n",
        "    plt.title('y = '+ str(img[1]))\n",
        "    plt.show()\n",
        "\n",
        "# We need to convert the images to numpy arrays as tensors are not compatible with matplotlib.\n",
        "def im_convert(tensor):\n",
        "    #Lets\n",
        "    img = tensor.cpu().clone().detach().numpy() #\n",
        "    img = img.transpose(1, 2, 0)\n",
        "    img = img * np.array(tuple(mean)) + np.array(tuple(std))\n",
        "    img = img.clip(0, 1) # Clipping the size to print the images later\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfQOxDnOvtbi"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Funcion para mostrar imagenes\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Tomar imagenes random\n",
        "data_iter = iter(trainloader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "# Mostramos imagenes\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "# Printeamos las etiquetas de las imagenes\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
        "\n",
        "image_size = images[0].shape\n",
        "image_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJr6JKNeuGNN"
      },
      "outputs": [],
      "source": [
        "show_data(trainset[8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBRC8KEduJjW"
      },
      "outputs": [],
      "source": [
        "# Different classes in CIPHAR 10 dataset.\n",
        "classes = ('airplane',\n",
        "           'automobile',\n",
        "           'bird',\n",
        "           'cat',\n",
        "           'deer',\n",
        "           'dog',\n",
        "           'frog',\n",
        "           'horse',\n",
        "           'ship',\n",
        "           'truck')\n",
        "\n",
        "# Define an iterable on the data\n",
        "data_iterable = iter(trainloader) # converting our train_dataloader to iterable so that we can iter through it.\n",
        "images, labels = next(data_iterable) #going from 1st batch of 100 images to the next batch\n",
        "fig = plt.figure(figsize=(20, 10))\n",
        "\n",
        "# Lets plot 50 images from our train_dataset\n",
        "for idx in np.arange(10):\n",
        "    ax = fig.add_subplot(2, 5, idx+1, xticks=[], yticks=[])\n",
        "\n",
        "    # Note: imshow cant print tensor !\n",
        "    # Lets convert tensor image to numpy using im_convert function for imshow to print the image\n",
        "    plt.imshow(im_convert(images[idx]))\n",
        "    ax.set_title(classes[labels[idx].item()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsgLRqfcvtbi"
      },
      "source": [
        "## 2. Arquitectura\n",
        "Realizar experimentos variando cantidad de capas densas, nodos, hidden layers y reportar el mejor y peor experimento. ¿Qué estrategia utilizaron? ¿Qué resultados obtuvieron?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BZm_gwNvtbi"
      },
      "outputs": [],
      "source": [
        "if experiment_name == \"MLP\" and arquitectura:\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.fc1 = nn.Linear(32*32*3, 32*32*3)\n",
        "            self.fc21 = nn.Linear(32*32*3, 32*32)\n",
        "            self.fc22 = nn.Linear(32*32, 120)\n",
        "            self.fc23 = nn.Linear(120, 120)\n",
        "            self.fc2 = nn.Linear(120, 84)\n",
        "            self.fc3 = nn.Linear(84, num_classes) # termina con 10 para quedarse con la mejor\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc21(x))\n",
        "            x = F.relu(self.fc22(x))\n",
        "            x = F.relu(self.fc23(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "            return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUf9UpTyvtbj"
      },
      "outputs": [],
      "source": [
        "if experiment_name == \"fc_hl_bigvalues\"and arquitectura:\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.fc1 = nn.Linear(32*32*3, 32*32*3)\n",
        "            self.fc21 = nn.Linear(32*32*3, 32*32)\n",
        "            self.fc22 = nn.Linear(32*32, 150)\n",
        "            self.fc23 = nn.Linear(150, 150)\n",
        "            self.fc2 = nn.Linear(150, 100)\n",
        "            self.fc3 = nn.Linear(100, num_classes) # termina con 10 para quedarse con la mejor\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc21(x))\n",
        "            x = F.relu(self.fc22(x))\n",
        "            x = F.relu(self.fc23(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "            return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4Xp9SPUvtbj"
      },
      "outputs": [],
      "source": [
        "if experiment_name == \"fc_hl_1layers\" and arquitectura:\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.fc1 = nn.Linear(32*32*3, 32*32*3)\n",
        "            self.fc21 = nn.Linear(32*32*3, 32*32)\n",
        "            self.fc22 = nn.Linear(32*32, 300)\n",
        "            self.fc23 = nn.Linear(300, 200)\n",
        "            self.fc_hidden = nn.Linear(200, 120)  # Nueva capa oculta con 120 nodos\n",
        "            self.fc2 = nn.Linear(120, 84)\n",
        "            self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc21(x))\n",
        "            x = F.relu(self.fc22(x))\n",
        "            x = F.relu(self.fc23(x))\n",
        "            x = F.relu(self.fc_hidden(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8Fm3kWPvtbj"
      },
      "outputs": [],
      "source": [
        "if experiment_name == \"fc_hl_bigvalues_and_2layers\" and arquitectura:\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.fc1 = nn.Linear(32*32*3, 32*32*3)\n",
        "            self.fc21 = nn.Linear(32*32*3, 32*32)\n",
        "            self.fc22 = nn.Linear(32*32, 300)\n",
        "            self.fc23 = nn.Linear(300, 200)\n",
        "            self.fc_hidden12 = nn.Linear(200, 200)  # Nueva capa oculta con 120 nodos\n",
        "            self.fc2 = nn.Linear(200, 120)\n",
        "            self.fc_hidden23 = nn.Linear(120, 84)\n",
        "            self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc21(x))\n",
        "            x = F.relu(self.fc22(x))\n",
        "            x = F.relu(self.fc23(x))\n",
        "            x = F.relu(self.fc_hidden12(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = F.relu(self.fc_hidden23(x))\n",
        "            x = self.fc3(x)\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gsf2GDivtbj"
      },
      "outputs": [],
      "source": [
        "if experiment_name == \"fc_hl_less_values\" and arquitectura:\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.fc1 = nn.Linear(32*32*3, 32*32*3)\n",
        "            self.fc21 = nn.Linear(32*32*3, 32*32)\n",
        "            self.fc22 = nn.Linear(32*32, 50)\n",
        "            self.fc23 = nn.Linear(50, 50)\n",
        "            self.fc2 = nn.Linear(50, 25)\n",
        "            self.fc3 = nn.Linear(25, num_classes) # termina con 10 para quedarse con la mejor\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc21(x))\n",
        "            x = F.relu(self.fc22(x))\n",
        "            x = F.relu(self.fc23(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoUOsU4Svtbk"
      },
      "outputs": [],
      "source": [
        "if experiment_name == \"fc_hl_1layer_less\" and arquitectura:\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.fc1 = nn.Linear(32*32*3, 32*32*3)\n",
        "            self.fc21 = nn.Linear(32*32*3, 32*32)\n",
        "            self.fc22 = nn.Linear(32*32, 120)\n",
        "            self.fc2 = nn.Linear(120, 84)\n",
        "            self.fc3 = nn.Linear(84, num_classes) # termina con 10 para quedarse con la mejor\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc21(x))\n",
        "            x = F.relu(self.fc22(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhubPvdEvtbk"
      },
      "outputs": [],
      "source": [
        "if experiment_name == \"fc_hl_1layerless_smallvalues\" and arquitectura:\n",
        "    class Net(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.fc1 = nn.Linear(32*32*3, 32*32*3)\n",
        "            self.fc21 = nn.Linear(32*32*3, 32*32)\n",
        "            self.fc22 = nn.Linear(32*32, 50)\n",
        "            self.fc2 = nn.Linear(50, 25)\n",
        "            self.fc3 = nn.Linear(25, num_classes) # termina con 10 para quedarse con la mejor\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc21(x))\n",
        "            x = F.relu(self.fc22(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "            return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq3foxLKvtbk"
      },
      "outputs": [],
      "source": [
        "for i in name_deep_net_exp:\n",
        "    if experiment_name == i and arquitectura:\n",
        "      step = 2\n",
        "      net = Net()\n",
        "      net.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slzn6-pqvtbk"
      },
      "source": [
        "## 3. Arquitectura CNN\n",
        "Extienda el análisis utilizando capas convolucionales y reportar el mejor y peor experimento. ¿Cómo se compara?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ5aOd5Gvtbl"
      },
      "outputs": [],
      "source": [
        "if experiment_name == \"cnn_lessvalues\" and arquitectura_cnn:\n",
        "    class NetConv(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.conv1 = nn.Conv2d(in_channels = 3, x_channels = 6,kernel_size = 5, stride = 1, padding = 1)\n",
        "            self.pool = nn.MaxPool2d(2, 2)\n",
        "            self.conv2 = nn.Conv2d(in_channels = 6, x_channels = 16, kernel_size = 5, stride = 1, padding = 1)\n",
        "            self.fc1 = nn.Linear(16 * 5 * 5, 50)\n",
        "            self.fc21 = nn.Linear(50, 50)\n",
        "            self.fc22 = nn.Linear(50, 50)\n",
        "            self.fc23 = nn.Linear(50, 50)\n",
        "            self.fc2 = nn.Linear(50, 25)\n",
        "            self.fc3 = nn.Linear(25, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.pool(F.relu(self.conv1(x)))\n",
        "            x = self.pool(F.relu(self.conv2(x)))\n",
        "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc21(x))\n",
        "            x = F.relu(self.fc22(x))\n",
        "            x = F.relu(self.fc23(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0enZAOabqmY7"
      },
      "outputs": [],
      "source": [
        "if experiment_name == \"cnn_3convlayers\"and arquitectura_cnn:\n",
        "    class NetConv(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.conv1 = nn.Conv2d(in_channels = 3, x_channels = 6,kernel_size = 5, stride = 1, padding = 1)\n",
        "            self.pool = nn.MaxPool2d(2, 2)\n",
        "            self.conv2 = nn.Conv2d(in_channels = 6, x_channels = 16, kernel_size = 2, stride = 2, padding = 1)\n",
        "            self.pool1 = nn.MaxPool2d(2, 2)\n",
        "            self.conv3 = nn.Conv2d(in_channels = 6, x_channels = 16, kernel_size = 2, stride = 1, padding = 1)\n",
        "            self.fc1 = nn.Linear(16 * 5 * 5, 50)\n",
        "            self.fc2 = nn.Linear(50, 50)\n",
        "            self.fc3 = nn.Linear(50, 50)\n",
        "            self.out = nn.Linear(25, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.pool(F.relu(self.conv1(x)))\n",
        "            x = self.pool(F.relu(self.conv2(x)))\n",
        "            x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = F.relu(self.fc3(x))\n",
        "            x = self.out(x)\n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPLJuowxvtbl"
      },
      "outputs": [],
      "source": [
        "\n",
        "if experiment_name == \"CNN_LeNet-5\"and arquitectura_cnn:\n",
        "    class NetConv(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(NetConv, self).__init__()\n",
        "            self.conv1 = nn.Conv2d(in_channels=3, x_channels=32, kernel_size=5, padding=2)\n",
        "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.conv2 = nn.Conv2d(in_channels=32, x_channels=48, kernel_size=5)\n",
        "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "            self.fc1 = nn.Linear(1728, 256)  # Adjusted xput size to 256\n",
        "            self.fc2 = nn.Linear(256, 84)\n",
        "            self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.pool1(F.relu(self.conv1(x)))\n",
        "            x = self.pool2(F.relu(self.conv2(x)))\n",
        "            x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = F.relu(self.fc2(x))\n",
        "            x = self.fc3(x)\n",
        "            return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejGmLqvkvtbl"
      },
      "outputs": [],
      "source": [
        "if experiment_name == \"CNN_ResNet-18\"and arquitectura_cnn:\n",
        "  class BasicBlock(nn.Module):\n",
        "      expansion = 1\n",
        "\n",
        "      def __init__(self, in_planes, planes, stride=1):\n",
        "          super(BasicBlock, self).__init__()\n",
        "          self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "          self.bn1 = nn.BatchNorm2d(planes)\n",
        "          self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "          self.bn2 = nn.BatchNorm2d(planes)\n",
        "          self.shortcut = nn.Sequential()\n",
        "          if stride != 1 or in_planes != self.expansion * planes:\n",
        "              self.shortcut = nn.Sequential(\n",
        "                  nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                  nn.BatchNorm2d(self.expansion * planes)\n",
        "              )\n",
        "\n",
        "      def forward(self, x):\n",
        "          out = F.relu(self.bn1(self.conv1(x)))\n",
        "          out = self.bn2(self.conv2(out))\n",
        "          out += self.shortcut(x)\n",
        "          out = F.relu(out)\n",
        "          return out\n",
        "\n",
        "  class NetConv(nn.Module):\n",
        "      def __init__(self, num_classes=10):\n",
        "          super(NetConv, self).__init__()\n",
        "          self.in_planes = 64\n",
        "\n",
        "          self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "          self.bn1 = nn.BatchNorm2d(64)\n",
        "          self.layer1 = self._make_layer(BasicBlock, 64, 2, stride=1)\n",
        "          self.layer2 = self._make_layer(BasicBlock, 128, 2, stride=2)\n",
        "          self.layer3 = self._make_layer(BasicBlock, 256, 2, stride=2)\n",
        "          self.layer4 = self._make_layer(BasicBlock, 512, 2, stride=2)\n",
        "          self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "          self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "      def _make_layer(self, block, planes, num_blocks, stride):\n",
        "          strides = [stride] + [1] * (num_blocks - 1)\n",
        "          layers = []\n",
        "          for stride in strides:\n",
        "              layers.append(block(self.in_planes, planes, stride))\n",
        "              self.in_planes = planes * block.expansion\n",
        "          return nn.Sequential(*layers)\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = F.relu(self.bn1(self.conv1(x)))\n",
        "          x = self.layer1(x)\n",
        "          x = self.layer2(x)\n",
        "          x = self.layer3(x)\n",
        "          x = self.layer4(x)\n",
        "          x = self.avgpool(x)\n",
        "          x = x.view(x.size(0), -1)\n",
        "          x = self.fc(x)\n",
        "          return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBI43Ljivtbm"
      },
      "outputs": [],
      "source": [
        "if experiment_name == \"CNN_VGG16\"and arquitectura_cnn:\n",
        "    import torchvision.models as models\n",
        "    # class NetConv(nn.Module):\n",
        "    #     def __init__(self):\n",
        "    #         super(NetConv, self).__init__()\n",
        "    #         self.vgg16 = models.vgg16(pretrained=False)\n",
        "    #         num_ftrs = self.vgg16.classifier[6].in_features\n",
        "    #         self.vgg16.classifier[6] = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "    #     def forward(self, x):\n",
        "    #         return self.vgg16(x)\n",
        "\n",
        "    class NetConv(nn.Module):\n",
        "        def __init__(self, num_classes=10):\n",
        "            super(NetConv, self).__init__()\n",
        "\n",
        "            self.transform = transforms.Compose(\n",
        "                [\n",
        "                    transforms.Resize((224, 224)),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            self.layer1 = nn.Sequential(\n",
        "                nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU())\n",
        "            self.layer2 = nn.Sequential(\n",
        "                nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "            self.layer3 = nn.Sequential(\n",
        "                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.ReLU())\n",
        "            self.layer4 = nn.Sequential(\n",
        "                nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "            self.layer5 = nn.Sequential(\n",
        "                nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.ReLU())\n",
        "            self.layer6 = nn.Sequential(\n",
        "                nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.ReLU())\n",
        "            self.layer7 = nn.Sequential(\n",
        "                nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "            self.layer8 = nn.Sequential(\n",
        "                nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.ReLU())\n",
        "            self.layer9 = nn.Sequential(\n",
        "                nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.ReLU())\n",
        "            self.layer10 = nn.Sequential(\n",
        "                nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "            self.layer11 = nn.Sequential(\n",
        "                nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.ReLU())\n",
        "            self.layer12 = nn.Sequential(\n",
        "                nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.ReLU())\n",
        "\n",
        "            self.layer13 = nn.Sequential(\n",
        "                nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "\n",
        "            self.fc = nn.Sequential(\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(512, 4096),\n",
        "                nn.ReLU())\n",
        "\n",
        "            self.fc1 = nn.Sequential(\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(4096, 4096),\n",
        "                nn.ReLU())\n",
        "            self.fc2= nn.Sequential(\n",
        "                nn.Linear(4096, num_classes))\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.layer1(x)\n",
        "            x = self.layer2(x)\n",
        "            x = self.layer3(x)\n",
        "            x = self.layer4(x)\n",
        "            x = self.layer5(x)\n",
        "            x = self.layer6(x)\n",
        "            x = self.layer7(x)\n",
        "            x = self.layer8(x)\n",
        "            x = self.layer9(x)\n",
        "            x = self.layer10(x)\n",
        "            x = self.layer11(x)\n",
        "            x = self.layer12(x)\n",
        "            x = self.layer13(x)\n",
        "            x = x.reshape(x.size(0), -1)\n",
        "            x = self.fc(x)\n",
        "            x = self.fc1(x)\n",
        "            x = self.fc2(x)\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHTXZKj8qmY8"
      },
      "outputs": [],
      "source": [
        "if experiment_name == \"CNN_SqueezeNet\"and arquitectura_cnn:\n",
        "    class FireModule(nn.Module):\n",
        "        def __init__(self, in_channels, s1x1, e1x1, e3x3):\n",
        "            super(FireModule, self).__init__()\n",
        "            self.squeeze = nn.Conv2d(in_channels=in_channels, out_channels=s1x1, kernel_size=1, stride=1)\n",
        "            self.expand1x1 = nn.Conv2d(in_channels=s1x1, out_channels=e1x1, kernel_size=1)\n",
        "            self.expand3x3 = nn.Conv2d(in_channels=s1x1, out_channels=e3x3, kernel_size=3, padding=1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = F.relu(self.squeeze(x))\n",
        "            x1 = self.expand1x1(x)\n",
        "            x2 = self.expand3x3(x)\n",
        "            x = F.relu(torch.cat((x1, x2), dim=1))\n",
        "            return x\n",
        "    class NetConv(nn.Module):\n",
        "        def __init__(self, out_channels):\n",
        "            super(NetConv, self).__init__()\n",
        "            self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=7, stride=2)\n",
        "            self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "            self.fire2 = FireModule(in_channels=96, s1x1=16, e1x1=64, e3x3=64)\n",
        "            self.fire3 = FireModule(in_channels=128, s1x1=16, e1x1=64, e3x3=64)\n",
        "            self.fire4 = FireModule(in_channels=128, s1x1=32, e1x1=128, e3x3=128)\n",
        "            self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "            self.fire5 = FireModule(in_channels=256, s1x1=32, e1x1=128, e3x3=128)\n",
        "            self.fire6 = FireModule(in_channels=256, s1x1=48, e1x1=192, e3x3=192)\n",
        "            self.fire7 = FireModule(in_channels=384, s1x1=48, e1x1=192, e3x3=192)\n",
        "            self.fire8 = FireModule(in_channels=384, s1x1=64, e1x1=256, e3x3=256)\n",
        "            self.max_pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "            self.fire9 = FireModule(in_channels=512, s1x1=64, e1x1=256, e3x3=256)\n",
        "            self.dropout = nn.Dropout(p=0.5)\n",
        "            self.conv10 = nn.Conv2d(in_channels=512, out_channels=out_channels, kernel_size=1, stride=1)\n",
        "            self.avg_pool = nn.AvgPool2d(kernel_size=12, stride=1)\n",
        "            # We don't have FC Layers, inspired by NiN architecture.\n",
        "\n",
        "        def forward(self, x):\n",
        "            # First max pool after conv1\n",
        "            x = self.max_pool1(self.conv1(x))\n",
        "            # Second max pool after fire4\n",
        "            x = self.max_pool2(self.fire4(self.fire3(self.fire2(x))))\n",
        "            # Third max pool after fire8\n",
        "            x = self.max_pool3(self.fire8(self.fire7(self.fire6(self.fire5(x)))))\n",
        "            # Final pool (avg in this case) after conv10\n",
        "            x = self.avg_pool(self.conv10(self.fire9(x)))\n",
        "            return torch.flatten(x, start_dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQSEZSebvtbm"
      },
      "outputs": [],
      "source": [
        "for i in name_conv_net_exp:\n",
        "    if experiment_name == i and arquitectura_cnn:\n",
        "      step = 3\n",
        "      # net = NetConv(num_classes)\n",
        "      net = NetConv()\n",
        "      net.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOHGXsXBvtbn"
      },
      "source": [
        "## 4. Funciones de activacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLe6qX77vtbn"
      },
      "source": [
        "vamos a utilizar ResNet-18 la cal presente mejor rendimiento de las arquitecturas CNN. Ahora en esta etapa ba vamos a experimentar con otra funciones de activacion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmXo-0ZXqmY8"
      },
      "outputs": [],
      "source": [
        "experiment_name = 'CNN_ResNet-18'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcWSYh2evtbn"
      },
      "outputs": [],
      "source": [
        "funcion_activacion = 'relu6'\n",
        "#leaky_relu\n",
        "#relu\n",
        "#relu6\n",
        "#elu\n",
        "#tanh\n",
        "#silu\n",
        "#sigmoid\n",
        "#softmax\n",
        "\n",
        "\n",
        "if experiment_name == \"CNN_ResNet-18\" and activacion_experiment:\n",
        "    step = 4\n",
        "    funcion_activacion = 'relu6'\n",
        "    class BasicBlock(nn.Module):\n",
        "        expansion = 1\n",
        "\n",
        "\n",
        "        def __init__(self, in_planes, planes, stride=1):\n",
        "            super(BasicBlock, self).__init__()\n",
        "\n",
        "            DROPOUT = 0.1 # aplicamos Droput del 10% de las neuronas\n",
        "\n",
        "            self.conv1 = nn.Conv2d(\n",
        "                in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(planes)\n",
        "            self.dropout = nn.Dropout(DROPOUT)\n",
        "            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                                  stride=1, padding=1, bias=False)\n",
        "            self.bn2 = nn.BatchNorm2d(planes)\n",
        "            self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "            self.shortcut = nn.Sequential()\n",
        "            if stride != 1 or in_planes != self.expansion*planes:\n",
        "                self.shortcut = nn.Sequential(\n",
        "                    nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                              kernel_size=1, stride=stride, bias=False),\n",
        "                    nn.BatchNorm2d(self.expansion*planes),\n",
        "                    nn.Dropout(DROPOUT)\n",
        "                )\n",
        "\n",
        "        def forward(self, x):\n",
        "            out = F.relu6(self.dropout(self.bn1(self.conv1(x))))\n",
        "            out = self.dropout(self.bn2(self.conv2(out)))\n",
        "            out += self.shortcut(x)\n",
        "            out = F.relu6(out)\n",
        "            return out\n",
        "\n",
        "\n",
        "    class ResNet(nn.Module):\n",
        "        def __init__(self, block, num_blocks, num_classes=10):\n",
        "            super(ResNet, self).__init__()\n",
        "            self.in_planes = 64\n",
        "\n",
        "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                                  stride=1, padding=1, bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(64)\n",
        "            self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "            self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "            self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "            self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "            self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "        def _make_layer(self, block, planes, num_blocks, stride):\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            layers = []\n",
        "            for stride in strides:\n",
        "                layers.append(block(self.in_planes, planes, stride))\n",
        "                self.in_planes = planes * block.expansion\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        def forward(self, x):\n",
        "            out = F.relu6(self.bn1(self.conv1(x)))\n",
        "            out = self.layer1(out)\n",
        "            out = self.layer2(out)\n",
        "            out = self.layer3(out)\n",
        "            out = self.layer4(out)\n",
        "            out = F.avg_pool2d(out, 4)\n",
        "            out = out.view(out.size(0), -1)\n",
        "            out = self.linear(out)\n",
        "            return F.log_softmax(out, dim=-1)\n",
        "\n",
        "\n",
        "    def ResNet18():\n",
        "        return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "    net = ResNet18().to(device)\n",
        "    summary(net, input_size=(3,32,32))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9DXbZ44vtbn"
      },
      "source": [
        "## 5. Optimizadores\n",
        "Realizar experimentos evaluando distintos optimizadores, schedulers y reportar el mejor y peor experimento. ¿Qué estrategia utilizaron? ¿Qué resultados obtuvieron?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShJB0KFLvtbn"
      },
      "outputs": [],
      "source": [
        "if optimizar:\n",
        "    step = 5\n",
        "    # funcion_activacion = 'leaky_relu'\n",
        "    ## Parámetros de entrenamiento\n",
        "    learning_rate = 0.01\n",
        "    momentum = 0.9\n",
        "    epochs = 10\n",
        "\n",
        "    # Optimizador y función de pérdida\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
        "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    # optimizer = optim.Adagrad(net.parameters(), lr=learning_rate, lr_decay=0.005, weight_decay=0.005)\n",
        "    # optimizer = optim.RMSprop(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=0.005)\n",
        "    # optimizer = optim.Adadelta(net.parameters(), lr=learning_rate, rho=0.9, eps=1e-06, weight_decay=0.005)\n",
        "\n",
        "\n",
        "    scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n",
        "    # scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
        "    # scheduler1 = ReduceLROnPlateau(optimizer, mode='min', factor=0.05, patience=2, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n",
        "    # scheduler1 = StepLR(optimizer, step_size=15, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeAJkemu_ckn"
      },
      "source": [
        "## 6. Entrenamiento - Mejora de parametros (lr, epochs, batchsize)\n",
        "Realizar experimentos evaluando distintos batch-sizes, epochs y reportar el mejor y peor experimento. ¿Qué resultados obtuvieron?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYb4Y3CVYzVw"
      },
      "source": [
        "Intetno de usar Sweeps, la configuracion seria la siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWFY9xjNYHio"
      },
      "outputs": [],
      "source": [
        "# if mejoras_lr:\n",
        "#     sweep_config = {\n",
        "#     'method': 'random'\n",
        "#     }\n",
        "\n",
        "#     metric = {\n",
        "#     'name': 'loss',\n",
        "#     'goal': 'minimize'\n",
        "#     }\n",
        "\n",
        "#     sweep_config['metric'] = metric\n",
        "\n",
        "#     parameters_dict = {\n",
        "#     'optimizer': {\n",
        "#         'values': ['adam', 'sgd']\n",
        "#         },\n",
        "#     'dropout': {\n",
        "#           'values': [0.1,0.3, 0.4, 0.5]\n",
        "#         },\n",
        "#     }\n",
        "\n",
        "#     parameters_dict.update({\n",
        "#     'epochs': {\n",
        "#         'value': 30}\n",
        "#     })\n",
        "\n",
        "#     parameters_dict.update({\n",
        "#     'learning_rate': {\n",
        "#         # a flat distribution between 0 and 0.1\n",
        "#         'distribution': 'uniform',\n",
        "#         'min': 0,\n",
        "#         'max': 0.1\n",
        "#         },\n",
        "#     'batch_size': {\n",
        "#         # integers between 32 and 256\n",
        "#         # with evenly-distributed logarithms\n",
        "#         'distribution': 'q_log_uniform_values', #q_log_uniform\n",
        "#         'q': 1, #8\n",
        "#         'min': 32,\n",
        "#         'max': 256,\n",
        "#         }\n",
        "#       })\n",
        "\n",
        "#     import pprint\n",
        "\n",
        "#     pprint.pprint(sweep_config)\n",
        "# if mejoras_lr:\n",
        "\n",
        "#   sweep_id = wandb.sweep(sweep_config, project=\"pytorch-sweeps-demo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EXFMbEtkOFCL",
        "outputId": "425f6140-76fb-42fa-eb7e-594fee55f2da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 0.005 0.8 1e-05 CNN_ResNet-18 256\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231106_000851-elnzg4kx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/chicas_superpoderosas/TP3/runs/elnzg4kx' target=\"_blank\">1.CNN_ResNet-18 relu6 SGD lr=0.005 batch_size=256 weight_decay=1e-05</a></strong> to <a href='https://wandb.ai/chicas_superpoderosas/TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/chicas_superpoderosas/TP3' target=\"_blank\">https://wandb.ai/chicas_superpoderosas/TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/chicas_superpoderosas/TP3/runs/elnzg4kx' target=\"_blank\">https://wandb.ai/chicas_superpoderosas/TP3/runs/elnzg4kx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name 'scheduler1' is not defined\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▃▄▅▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▆▆▇▇▇█</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>74.63</td></tr><tr><td>train_loss</td><td>0.00283</td></tr><tr><td>val_accuracy</td><td>74.37</td></tr><tr><td>val_loss</td><td>0.00294</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">1.CNN_ResNet-18 relu6 SGD lr=0.005 batch_size=256 weight_decay=1e-05</strong> at: <a href='https://wandb.ai/chicas_superpoderosas/TP3/runs/elnzg4kx' target=\"_blank\">https://wandb.ai/chicas_superpoderosas/TP3/runs/elnzg4kx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20231106_000851-elnzg4kx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15 0.05 0.6 0.01 CNN_ResNet-18 32\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231106_001614-5kndv0ul</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/chicas_superpoderosas/TP3/runs/5kndv0ul' target=\"_blank\">2.CNN_ResNet-18 relu6 SGD lr=0.05 batch_size=32 weight_decay=0.01</a></strong> to <a href='https://wandb.ai/chicas_superpoderosas/TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/chicas_superpoderosas/TP3' target=\"_blank\">https://wandb.ai/chicas_superpoderosas/TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/chicas_superpoderosas/TP3/runs/5kndv0ul' target=\"_blank\">https://wandb.ai/chicas_superpoderosas/TP3/runs/5kndv0ul</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   200] loss: 0.222\n",
            "[1,   400] loss: 0.398\n",
            "[1,   600] loss: 0.564\n",
            "[1,   800] loss: 0.723\n",
            "[1,  1000] loss: 0.874\n",
            "[1,  1200] loss: 1.020\n",
            "[2,   200] loss: 0.139\n",
            "[2,   400] loss: 0.278\n",
            "[2,   600] loss: 0.412\n",
            "[2,   800] loss: 0.544\n",
            "[2,  1000] loss: 0.679\n",
            "[2,  1200] loss: 0.810\n",
            "[3,   200] loss: 0.130\n",
            "[3,   400] loss: 0.258\n",
            "[3,   600] loss: 0.388\n",
            "[3,   800] loss: 0.517\n",
            "[3,  1000] loss: 0.643\n",
            "[3,  1200] loss: 0.771\n",
            "[4,   200] loss: 0.125\n",
            "[4,   400] loss: 0.247\n",
            "[4,   600] loss: 0.369\n",
            "[4,   800] loss: 0.492\n",
            "[4,  1000] loss: 0.613\n",
            "[4,  1200] loss: 0.731\n",
            "[5,   200] loss: 0.116\n",
            "[5,   400] loss: 0.234\n",
            "[5,   600] loss: 0.352\n",
            "[5,   800] loss: 0.468\n",
            "[5,  1000] loss: 0.586\n",
            "[5,  1200] loss: 0.703\n",
            "[6,   200] loss: 0.112\n",
            "[6,   400] loss: 0.226\n",
            "[6,   600] loss: 0.341\n",
            "[6,   800] loss: 0.456\n",
            "[6,  1000] loss: 0.569\n",
            "[6,  1200] loss: 0.683\n",
            "[7,   200] loss: 0.114\n",
            "[7,   400] loss: 0.226\n",
            "[7,   600] loss: 0.339\n",
            "[7,   800] loss: 0.452\n",
            "[7,  1000] loss: 0.565\n",
            "[7,  1200] loss: 0.676\n",
            "[8,   200] loss: 0.110\n",
            "[8,   400] loss: 0.221\n",
            "[8,   600] loss: 0.331\n",
            "[8,   800] loss: 0.444\n",
            "[8,  1000] loss: 0.554\n",
            "[8,  1200] loss: 0.666\n",
            "[9,   200] loss: 0.112\n",
            "[9,   400] loss: 0.222\n",
            "[9,   600] loss: 0.333\n",
            "[9,   800] loss: 0.442\n",
            "[9,  1000] loss: 0.554\n",
            "[9,  1200] loss: 0.664\n",
            "[10,   200] loss: 0.110\n",
            "[10,   400] loss: 0.219\n",
            "[10,   600] loss: 0.328\n",
            "[10,   800] loss: 0.438\n",
            "[10,  1000] loss: 0.551\n",
            "[10,  1200] loss: 0.662\n",
            "[11,   200] loss: 0.108\n",
            "[11,   400] loss: 0.217\n",
            "[11,   600] loss: 0.326\n",
            "[11,   800] loss: 0.434\n",
            "[11,  1000] loss: 0.544\n",
            "[11,  1200] loss: 0.653\n",
            "[12,   200] loss: 0.109\n",
            "[12,   400] loss: 0.218\n",
            "[12,   600] loss: 0.329\n",
            "[12,   800] loss: 0.435\n",
            "[12,  1000] loss: 0.546\n",
            "[12,  1200] loss: 0.655\n",
            "[13,   200] loss: 0.111\n",
            "[13,   400] loss: 0.221\n",
            "[13,   600] loss: 0.329\n",
            "[13,   800] loss: 0.441\n",
            "[13,  1000] loss: 0.549\n",
            "[13,  1200] loss: 0.659\n",
            "[14,   200] loss: 0.111\n",
            "[14,   400] loss: 0.219\n",
            "[14,   600] loss: 0.326\n",
            "[14,   800] loss: 0.433\n",
            "[14,  1000] loss: 0.544\n",
            "[14,  1200] loss: 0.650\n",
            "[15,   200] loss: 0.109\n"
          ]
        }
      ],
      "source": [
        "if mejoras_lr:\n",
        "    step = 6\n",
        "    import random\n",
        "    from itertools import product\n",
        "\n",
        "    funcion_activacion = 'relu6'\n",
        "    class BasicBlock(nn.Module):\n",
        "        expansion = 1\n",
        "\n",
        "\n",
        "        def __init__(self, in_planes, planes, stride=1):\n",
        "            super(BasicBlock, self).__init__()\n",
        "\n",
        "            self.DROPOUT = 0.1 # aplicamos Droput del 10% de las neuronas\n",
        "\n",
        "            self.conv1 = nn.Conv2d(\n",
        "                in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(planes)\n",
        "            self.dropout = nn.Dropout(self.DROPOUT)\n",
        "            self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                                stride=1, padding=1, bias=False)\n",
        "            self.bn2 = nn.BatchNorm2d(planes)\n",
        "            self.dropout = nn.Dropout(self.DROPOUT)\n",
        "\n",
        "            self.shortcut = nn.Sequential()\n",
        "            if stride != 1 or in_planes != self.expansion*planes:\n",
        "                self.shortcut = nn.Sequential(\n",
        "                    nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                            kernel_size=1, stride=stride, bias=False),\n",
        "                    nn.BatchNorm2d(self.expansion*planes),\n",
        "                    nn.Dropout(self.DROPOUT)\n",
        "                )\n",
        "\n",
        "        def forward(self, x):\n",
        "            out = F.relu6(self.dropout(self.bn1(self.conv1(x))))\n",
        "            out = self.dropout(self.bn2(self.conv2(out)))\n",
        "            out += self.shortcut(x)\n",
        "            out = F.relu6(out)\n",
        "            return out\n",
        "\n",
        "\n",
        "    class ResNet(nn.Module):\n",
        "        def __init__(self, block, num_blocks, num_classes=10):\n",
        "            super(ResNet, self).__init__()\n",
        "            self.in_planes = 64\n",
        "\n",
        "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                                stride=1, padding=1, bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(64)\n",
        "            self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "            self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "            self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "            self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "            self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "        def _make_layer(self, block, planes, num_blocks, stride):\n",
        "            strides = [stride] + [1]*(num_blocks-1)\n",
        "            layers = []\n",
        "            for stride in strides:\n",
        "                layers.append(block(self.in_planes, planes, stride))\n",
        "                self.in_planes = planes * block.expansion\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "        def forward(self, x):\n",
        "            out = F.relu6(self.bn1(self.conv1(x)))\n",
        "            out = self.layer1(out)\n",
        "            out = self.layer2(out)\n",
        "            out = self.layer3(out)\n",
        "            out = self.layer4(out)\n",
        "            out = F.avg_pool2d(out, 4)\n",
        "            out = out.view(out.size(0), -1)\n",
        "            out = self.linear(out)\n",
        "            return F.log_softmax(out, dim=-1)\n",
        "\n",
        "\n",
        "    def ResNet18():\n",
        "        return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "    net = ResNet18().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if mejoras_lr:\n",
        "    # Define the range of hyperparameters you want to search over\n",
        "    learning_rate_range = [0.001, 0.002, 0.005,0.01,0.02,0.05,0.1]\n",
        "    momentum_range = [0.6 + i * 0.05 for i in range(6)]  # Creates [0.6, 0.65, 0.7, 0.75, 0.8, 0.85]\n",
        "    weight_decay_range = [1e-2, 1e-3, 1e-4, 1e-5]  # Adjust the values as needed\n",
        "    epochs_range = list(range(10, 30, 5))\n",
        "    batch_size_values = [8, 16, 32, 64, 128, 256, 512]\n",
        "\n",
        "    # Define the number of experiments you want to run\n",
        "    num_experiments = 10\n",
        "\n",
        "    # Wrap your existing code into a function\n",
        "\n",
        "    def train_and_evaluate(epochs, learning_rate, momentum, weight_decay, experiment_name, batch_size, epsilon=0.001):\n",
        "        trainset = create_trainset(root='./data', transform=composed_train)\n",
        "        trainloader, valloader = create_loaders(trainset, batch_size=batch_size)\n",
        "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "        scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Initialize early stopping parameters\n",
        "        best_val_loss = float('inf')\n",
        "        consecutive_no_improvement = 0\n",
        "\n",
        "        # Set up your wandb run here with the appropriate project and name\n",
        "        experiment_name = f\"{step}.{experiment_name} {funcion_activacion} {optimizer.__class__.__name__} lr={learning_rate} batch_size={batch_size} weight_decay={weight_decay}\"\n",
        "\n",
        "        try:\n",
        "            wandb.init(\n",
        "                project=project_name,\n",
        "                name=experiment_name,\n",
        "                config={\n",
        "                    \"learning_rate\": learning_rate,\n",
        "                    \"momentum\": momentum,\n",
        "                    \"batch_size\": batch_size,\n",
        "                    \"epochs\": epochs,\n",
        "                },\n",
        "            )\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                running_loss = 0.0\n",
        "                train_correct = 0\n",
        "                total = 0\n",
        "                for i, data in enumerate(trainloader, 0):\n",
        "                    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = net(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if regularizadores:\n",
        "                        loss = regularization(net, loss, l1_lambda=weight_decay, l2_lambda=weight_decay, L1=L1, L2=L2)\n",
        "\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item()\n",
        "                    if i % 200 == 199:\n",
        "                        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    total += labels.size(0)\n",
        "                    train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                scheduler1.step()\n",
        "\n",
        "                train_accuracy = 100 * train_correct / total\n",
        "                running_loss = running_loss / total\n",
        "\n",
        "                val_correct = 0\n",
        "                total = 0\n",
        "                val_loss = 0\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    for data in valloader:\n",
        "                        images, labels = data[0].to(device), data[1].to(device)\n",
        "                        outputs = net(images)\n",
        "                        _, predicted = torch.max(outputs.data, 1)\n",
        "                        total += labels.size(0)\n",
        "                        val_correct += (predicted == labels).sum().item()\n",
        "                        val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "                val_accuracy = 100 * val_correct / total\n",
        "                val_loss = val_loss / total\n",
        "\n",
        "                wandb.log({\n",
        "                    \"train_accuracy\": train_accuracy,\n",
        "                    \"val_accuracy\": val_accuracy,\n",
        "                    \"train_loss\": running_loss,\n",
        "                    \"val_loss\": val_loss\n",
        "                })\n",
        "\n",
        "                # Implement early stopping\n",
        "                if val_loss < best_val_loss - epsilon:\n",
        "                    best_val_loss = val_loss\n",
        "                    consecutive_no_improvement = 0\n",
        "                else:\n",
        "                    consecutive_no_improvement += 1\n",
        "\n",
        "                if consecutive_no_improvement >= 5:\n",
        "                    print(\"Early stopping: Validation loss did not improve for 5 consecutive epochs.\")\n",
        "                    break  # Stop training\n",
        "            #guardo el modelo\n",
        "            PATH = f'./{experiment_name}.pth'\n",
        "            metadata = generar_metadata(net,\n",
        "                                        experiment_name,\n",
        "                                        epochs,\n",
        "                                        learning_rate,\n",
        "                                        batch_size,\n",
        "                                        momentum,\n",
        "                                        optimizer,\n",
        "                                        scheduler1,\n",
        "                                        train_accuracy,\n",
        "                                        running_loss,\n",
        "                                        val_accuracy,\n",
        "                                        val_loss,\n",
        "                                        data_augmentation,\n",
        "                                        net.DROPOUT,\n",
        "                                        weight_decay,\n",
        "                                        regularizadores_data = {'data_augmentation': data_augmentation,'dropout': net.DROPOUT,'weight_decay': weight_decay,'L1':L1,'L2':L2})\n",
        "            save_data_to_json(metadata,PATH)\n",
        "            print(metadata)\n",
        "            print('Finished Training')\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "        finally:\n",
        "            wandb.finish()\n",
        "\n",
        "\n",
        "    # Create a list of hyperparameter combinations to try\n",
        "    hyperparameter_combinations = list(product(epochs_range,\n",
        "        learning_rate_range, momentum_range, weight_decay_range,batch_size_values))\n",
        "\n",
        "    # Shuffle the combinations for random search\n",
        "    random.shuffle(hyperparameter_combinations)\n",
        "\n",
        "    for step, (epochs, learning_rate, momentum, weight_decay,batch_size) in enumerate(hyperparameter_combinations[:num_experiments], 1):\n",
        "        print(epochs, learning_rate, momentum, weight_decay,experiment_name,batch_size)\n",
        "        train_and_evaluate(epochs, learning_rate, momentum, weight_decay,experiment_name,batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkQt8vYsvtb4"
      },
      "source": [
        "## 7. Regularización\n",
        "Evaluar alguna(s) técnica(s) de regularización. ¿Qué resultados obtuvieron?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEsQBfxNl0C9"
      },
      "source": [
        "a) L1 Regularization\n",
        "l1_penalty = torch.nn.L1Loss(size_average=False)\n",
        "reg_loss = 0\n",
        "for param in model.parameters():\n",
        "→reg_loss += l1_penalty(param)\n",
        "\n",
        "factor = const_val #lambda\n",
        "loss += factor * reg_loss\n",
        "\n",
        "b) L2 Regularization\n",
        "The weight_decay parameter applies L2 regularization while initialising optimizer. This adds regularization term to the loss function, with the effect of shrinking the parameter estimates, making the model simpler and less likely to overfit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daiTepcBYPcv"
      },
      "source": [
        "Para regularizar y evitar overfitting agregamos Dropout=0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VQw2uwnvtb4"
      },
      "outputs": [],
      "source": [
        "if experiment_name == \"CNN_ResNet-18\" and regularizadores:\n",
        "  funcion_activacion = 'relu6'\n",
        "  step = 7\n",
        "  class BasicBlock(nn.Module):\n",
        "      expansion = 1\n",
        "\n",
        "\n",
        "      def __init__(self, in_planes, planes, stride=1):\n",
        "          super(BasicBlock, self).__init__()\n",
        "\n",
        "          DROPOUT = 0.1 # aplicamos Droput del 10% de las neuronas\n",
        "\n",
        "          self.conv1 = nn.Conv2d(\n",
        "              in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "          self.bn1 = nn.BatchNorm2d(planes)\n",
        "          self.dropout = nn.Dropout(DROPOUT)\n",
        "          self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                                stride=1, padding=1, bias=False)\n",
        "          self.bn2 = nn.BatchNorm2d(planes)\n",
        "          self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "          self.shortcut = nn.Sequential()\n",
        "          if stride != 1 or in_planes != self.expansion*planes:\n",
        "              self.shortcut = nn.Sequential(\n",
        "                  nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                            kernel_size=1, stride=stride, bias=False),\n",
        "                  nn.BatchNorm2d(self.expansion*planes),\n",
        "                  nn.Dropout(DROPOUT)\n",
        "              )\n",
        "\n",
        "      def forward(self, x):\n",
        "          out = F.relu6(self.dropout(self.bn1(self.conv1(x))))\n",
        "          out = self.dropout(self.bn2(self.conv2(out)))\n",
        "          out += self.shortcut(x)\n",
        "          out = F.relu6(out)\n",
        "          return out\n",
        "\n",
        "\n",
        "  class ResNet(nn.Module):\n",
        "      def __init__(self, block, num_blocks, num_classes=10):\n",
        "          super(ResNet, self).__init__()\n",
        "          self.in_planes = 64\n",
        "\n",
        "          self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                                stride=1, padding=1, bias=False)\n",
        "          self.bn1 = nn.BatchNorm2d(64)\n",
        "          self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "          self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "          self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "          self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "          self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "      def _make_layer(self, block, planes, num_blocks, stride):\n",
        "          strides = [stride] + [1]*(num_blocks-1)\n",
        "          layers = []\n",
        "          for stride in strides:\n",
        "              layers.append(block(self.in_planes, planes, stride))\n",
        "              self.in_planes = planes * block.expansion\n",
        "          return nn.Sequential(*layers)\n",
        "\n",
        "      def forward(self, x):\n",
        "          out = F.relu6(self.bn1(self.conv1(x)))\n",
        "          out = self.layer1(out)\n",
        "          out = self.layer2(out)\n",
        "          out = self.layer3(out)\n",
        "          out = self.layer4(out)\n",
        "          out = F.avg_pool2d(out, 4)\n",
        "          out = out.view(out.size(0), -1)\n",
        "          out = self.linear(out)\n",
        "          return F.log_softmax(out, dim=-1)\n",
        "\n",
        "\n",
        "  def ResNet18():\n",
        "      return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "  net = ResNet18().to(device)\n",
        "  summary(net, input_size=(3,32,32))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiTDQFH8l5mG"
      },
      "outputs": [],
      "source": [
        "if regularizadores:\n",
        "    step = 7\n",
        "    funcion_activacion = 'relu6'\n",
        "    learning_rate = 0.001\n",
        "    momentum = 0.7\n",
        "    epochs = 15\n",
        "    weight_decay = 1e-5\n",
        "\n",
        "    # Optimizador y función de pérdida\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum,weight_decay = weight_decay)\n",
        "    # optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    # optimizer = optim.Adagrad(net.parameters(), lr=learning_rate, lr_decay=0.005, weight_decay=0.005)\n",
        "    # optimizer = optim.RMSprop(net.parameters(), lr=learning_rate, momentum=momentum, weight_decay=0.005)\n",
        "    # optimizer = optim.Adadelta(net.parameters(), lr=learning_rate, rho=0.9, eps=1e-06, weight_decay=0.005)\n",
        "\n",
        "    # scheduler1 = ReduceLROnPlateau(optimizer, mode='min', factor=0.05, patience=2, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n",
        "    # scheduler1 = StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "    scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n",
        "    # scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7O4Xq99hovc"
      },
      "outputs": [],
      "source": [
        "def regularization(net, loss, l1_lambda=0.0, l2_lambda=0.0, L1=False, L2=False):\n",
        "    if L1:\n",
        "        l1_norm = sum(param.abs().sum() for param in net.parameters())\n",
        "        loss = loss + l1_lambda * l1_norm\n",
        "    if L2:\n",
        "        l2_reg = 0.0\n",
        "        for param in net.parameters():\n",
        "            l2_reg += torch.norm(param, p=2)\n",
        "        loss = loss + l2_lambda * l2_reg\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVeDbm-ur2Oy"
      },
      "outputs": [],
      "source": [
        "L1 = False\n",
        "L2 = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6lPWVphmBcX"
      },
      "source": [
        "### Data aguemntation\n",
        "hecho en la parte 1. Configuracion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_9LEzhcvtb1"
      },
      "source": [
        "## 8. Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnGWVWj0vtb1"
      },
      "source": [
        "### Entrenamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgI_eIwXvtb1"
      },
      "outputs": [],
      "source": [
        "experiment_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvHcBio0vtb2"
      },
      "outputs": [],
      "source": [
        "experiment_name = str(step) +'.'+ experiment_name + \" \" +funcion_activacion +\" \"+ optimizer.__class__.__name__ +\" \"+ scheduler1.__class__.__name__\n",
        "experiment_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fndXKJmtvtb2"
      },
      "outputs": [],
      "source": [
        "wandb.finish()\n",
        "wandb.init(\n",
        "    # seteamos el projecto donde se va a guardar el experimento en wandb\n",
        "    project=project_name,\n",
        "    name=experiment_name,\n",
        "\n",
        "    # trackeamos los hiperparámetros y las métricas\n",
        "    config={\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"momentum\": momentum,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs,\n",
        "        },\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FD07JjAvtb2"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "        # ---------------- SECCION DE TRAIN -----------------\n",
        "        running_loss = 0.0\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            #inputs, labels = data\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            if regularizadores:\n",
        "                loss = regularization(net, loss, l1_lambda=weight_decay, l2_lambda=weight_decay, L1=L1, L2=L2)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 200 == 199:    # print every 200 mini-batches\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        scheduler1.step()\n",
        "\n",
        "        # End of test section\n",
        "        # Val section\n",
        "        train_accuracy = 100 * train_correct / total\n",
        "        running_loss = running_loss/total\n",
        "\n",
        "        # ---------------- SECCION DE VALIDACION ----------------\n",
        "        val_correct = 0\n",
        "        total = 0\n",
        "        val_loss = 0\n",
        "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "        with torch.no_grad():\n",
        "            for data in valloader:\n",
        "                images, labels = data[0].to(device), data[1].to(device)\n",
        "                # calculate outputs by running images through the network\n",
        "                outputs = net(images)\n",
        "                # the class with the highest energy is what we choose as prediction\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "        # End of test section\n",
        "\n",
        "        val_accuracy = 100 * val_correct / total\n",
        "        val_loss = val_loss / total\n",
        "\n",
        "        wandb.log({\n",
        "            \"train_accuracy\": train_accuracy,\n",
        "            \"val_accuracy\": val_accuracy,\n",
        "            \"train_loss\": running_loss,\n",
        "            \"val_loss\": val_loss})\n",
        "\n",
        "    print('Finished Training')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "finally:\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epr4YAsbvtb3"
      },
      "source": [
        "Ejemplo de guardar el modelo. Sin embargo lo deberiamos guardar para el que mejor dio en validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmIJdNjVvtb3"
      },
      "outputs": [],
      "source": [
        "PATH = f'./{experiment_name}.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgVzFwHBsooo"
      },
      "source": [
        "## El mejor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8qO-Sl6vtb4"
      },
      "source": [
        "## 9. Evaluación final\n",
        "Evaluar el mejor modelo, es decir, el mejor de los ejercicios anteriores, con el set de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8Sag_Zmvtb4"
      },
      "outputs": [],
      "source": [
        "#Cargamos el modelo (el mejor) y evaluamos en un ejemplo\n",
        "#net = Net()\n",
        "net = ResNet18().to(device)\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "dataiter = iter(testloader)\n",
        "data = next(dataiter)\n",
        "test_images, test_labels = data\n",
        "\n",
        "# print test_images\n",
        "imshow(torchvision.utils.make_grid(test_images))\n",
        "print(\"GroundTruth: \", \" \".join(f\"{classes[test_labels[j]]:5s}\" for j in range(batch_size)))\n",
        "\n",
        "test_images, test_labels = data[0].to(device), data[1].to(device)\n",
        "outputs = net(test_images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
        "                              for j in range(4)))\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        test_images, test_labels = data[0].to(device), data[1].to(device)\n",
        "        # calculate outputs by running test_images through the network\n",
        "        outputs = net(test_images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += test_labels.size(0)\n",
        "        correct += (predicted == test_labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the network on the 10000 test test_images: {100 * correct // total} %\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQDFnhkAvtb5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        test_images, testloader = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(test_images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(test_labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f\"Accuracy for class: {classname:5s} is {accuracy:.1f} %\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "vsgLRqfcvtbi",
        "Slzn6-pqvtbk",
        "v9DXbZ44vtbn",
        "IeAJkemu_ckn"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
